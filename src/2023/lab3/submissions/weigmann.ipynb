{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5AYt3Cn35zA"
      },
      "source": [
        "**Challenge: Implement a Multiclass Classification Neural Network using PyTorch**\n",
        "\n",
        "Objective:\n",
        "Build a feedforward neural network using PyTorch to predict the species of iris flowers in a multiclass classification problem. The dataset used for this challenge is the Iris dataset, which consists of features like sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. **Data Preparation**: Load the MNIST dataset using ```torchvision.datasets.MNIST```. Standardize/normalize the features. Split the dataset into training and testing sets using, for example, ```sklearn.model_selection.train_test_split()```. **Bonus scores**: *use PyTorch's built-* ```DataLoader``` *to split the dataset*.\n",
        "\n",
        "2. **Neural Network Architecture**: Define a simple feedforward neural network using PyTorch's ```nn.Module```. Design the input layer to match the number of features in the MNIST dataset and the output layer to have as many neurons as there are classes (10). You can experiment with the number of hidden layers and neurons to optimize the performance. **Bonus scores**: *Make your architecture flexibile to have as many hidden layers as the user wants, and use hyperparameter optimization to select the best number of hidden layeres.*\n",
        "\n",
        "3. **Loss Function and Optimizer**: Choose an appropriate loss function for multiclass classification. Select an optimizer, like SGD (Stochastic Gradient Descent) or Adam.\n",
        "\n",
        "4. **Training**: Write a training loop to iterate over the dataset.\n",
        "Forward pass the input through the network, calculate the loss, and perform backpropagation. Update the weights of the network using the chosen optimizer.\n",
        "\n",
        "5. **Testing**: Evaluate the trained model on the test set. Calculate the accuracy of the model.\n",
        "\n",
        "6. **Optimization**: Experiment with hyperparameters (learning rate, number of epochs, etc.) to optimize the model's performance. Consider adjusting the neural network architecture for better results. **Notice that you can't use the optimization algorithms from scikit-learn that we saw in lab1: e.g.,** ```GridSearchCV```.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5XLynrxJ33v6"
      },
      "outputs": [],
      "source": [
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([60000, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(trainset.data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size =32\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, drop_last=True, shuffle=True)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, drop_last=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MNISTConvNet(nn.Module):\n",
        "\n",
        "    def __init__(self, n_hiddenlayers, hiddenlayer_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=28, kernel_size=(3,3), stride=1, padding=1)\n",
        "\n",
        "        self.act1 = nn.ReLU()\n",
        "        self.drop1 = nn.Dropout()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=28, out_channels=28, kernel_size=(3,3), stride=1, padding=1)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        # (28, w, h)\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        self.fc3 = nn.Linear(5488, hiddenlayer_size)\n",
        "        self.act3 = nn.ReLU()\n",
        "\n",
        "        self.hidden_layers = []\n",
        "\n",
        "        for layer in range(n_hiddenlayers):\n",
        "            self.hidden_layers.append(nn.Linear(hiddenlayer_size, hiddenlayer_size))\n",
        "            self.hidden_layers.append(nn.ReLU())\n",
        "\n",
        "\n",
        "        self.fc4 = nn.Linear(hiddenlayer_size, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #input 1 x 28 x 28, output 28 x 28 x 28\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        #input 28 x 28 x 28, output 28 x 28 x 28\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        #input 28 x 28 x 28, output 28 x 14 x 14\n",
        "        x = self.pool2(x)\n",
        "        #input 28 x 14 x 14, 5488\n",
        "        x = self.flat(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.act3(x)\n",
        "\n",
        "        for element in self.hidden_layers:\n",
        "            x = element(x)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set 5 different models:\n",
        "1. 2 hidden layers with size 300\n",
        "2. 2 hidden layers with size 600\n",
        "3. 2 hidden layers with size 1000\n",
        "4. 3 hidden layers with size 600\n",
        "5. 4 hidden layers with size 600"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1 = MNISTConvNet(2, 300)\n",
        "model2 = MNISTConvNet(2, 600)\n",
        "model3 = MNISTConvNet(2, 1000)\n",
        "model4 = MNISTConvNet(3, 600)\n",
        "model5 = MNISTConvNet(4, 600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Loss Function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lr, momentum = 0.001, 0.9\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer1 = torch.optim.SGD(model1.parameters(), lr=lr, momentum=momentum)\n",
        "optimizer2 = torch.optim.SGD(model2.parameters(), lr=lr, momentum=momentum)\n",
        "optimizer3 = torch.optim.SGD(model3.parameters(), lr=lr, momentum=momentum)\n",
        "optimizer4 = torch.optim.SGD(model4.parameters(), lr=lr, momentum=momentum)\n",
        "optimizer5 = torch.optim.SGD(model5.parameters(), lr=lr, momentum=momentum)\n",
        "n_epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. / 5. / 6. Training and Testing, experimenting with different architectures and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run with learning rate 0.001 and momentum 0.9\n",
            "Epoch 1 --> loss model 1 = 0.03742362931370735\n",
            "Epoch 1 --> loss model 2 = 0.04632783308625221\n",
            "Epoch 1 --> loss model 3 = 0.03375035896897316\n",
            "Epoch 1 --> loss model 4 = 0.08205492794513702\n",
            "Epoch 1 --> loss model 5 = 0.12952156364917755\n",
            "Epoch 1 --> model 1 accuracy = 98.50761413574219\n",
            "Epoch 1 --> model 2 accuracy = 98.13702392578125\n",
            "Epoch 1 --> model 3 accuracy = 98.54767608642578\n",
            "Epoch 1 --> model 4 accuracy = 97.666259765625\n",
            "Epoch 1 --> model 5 accuracy = 96.42427825927734\n",
            "Epoch 2 --> loss model 1 = 0.033643849194049835\n",
            "Epoch 2 --> loss model 2 = 0.04251237213611603\n",
            "Epoch 2 --> loss model 3 = 0.029748613014817238\n",
            "Epoch 2 --> loss model 4 = 0.07441126555204391\n",
            "Epoch 2 --> loss model 5 = 0.12232403457164764\n",
            "Epoch 2 --> model 1 accuracy = 98.48757934570312\n",
            "Epoch 2 --> model 2 accuracy = 98.14703369140625\n",
            "Epoch 2 --> model 3 accuracy = 98.2371826171875\n",
            "Epoch 2 --> model 4 accuracy = 97.70632934570312\n",
            "Epoch 2 --> model 5 accuracy = 96.16386413574219\n",
            "Epoch 3 --> loss model 1 = 0.031138950958848\n",
            "Epoch 3 --> loss model 2 = 0.0382792130112648\n",
            "Epoch 3 --> loss model 3 = 0.026151062920689583\n",
            "Epoch 3 --> loss model 4 = 0.06852951645851135\n",
            "Epoch 3 --> loss model 5 = 0.11279653012752533\n",
            "Epoch 3 --> model 1 accuracy = 98.61778259277344\n",
            "Epoch 3 --> model 2 accuracy = 98.22715759277344\n",
            "Epoch 3 --> model 3 accuracy = 98.40745544433594\n",
            "Epoch 3 --> model 4 accuracy = 97.77644348144531\n",
            "Epoch 3 --> model 5 accuracy = 96.35417175292969\n",
            "Epoch 4 --> loss model 1 = 0.028564484789967537\n",
            "Epoch 4 --> loss model 2 = 0.03588055446743965\n",
            "Epoch 4 --> loss model 3 = 0.02449648082256317\n",
            "Epoch 4 --> loss model 4 = 0.06292381882667542\n",
            "Epoch 4 --> loss model 5 = 0.10439885407686234\n",
            "Epoch 4 --> model 1 accuracy = 98.57772827148438\n",
            "Epoch 4 --> model 2 accuracy = 98.53765869140625\n",
            "Epoch 4 --> model 3 accuracy = 98.53765869140625\n",
            "Epoch 4 --> model 4 accuracy = 98.056884765625\n",
            "Epoch 4 --> model 5 accuracy = 96.73477172851562\n",
            "Epoch 5 --> loss model 1 = 0.0263015516102314\n",
            "Epoch 5 --> loss model 2 = 0.03244421258568764\n",
            "Epoch 5 --> loss model 3 = 0.021008526906371117\n",
            "Epoch 5 --> loss model 4 = 0.05738968774676323\n",
            "Epoch 5 --> loss model 5 = 0.09599084407091141\n",
            "Epoch 5 --> model 1 accuracy = 98.6678695678711\n",
            "Epoch 5 --> model 2 accuracy = 98.65785217285156\n",
            "Epoch 5 --> model 3 accuracy = 98.6278076171875\n",
            "Epoch 5 --> model 4 accuracy = 97.76642608642578\n",
            "Epoch 5 --> model 5 accuracy = 96.22395324707031\n",
            "Run with learning rate 0.01 and momentum 0.9\n",
            "Epoch 1 --> loss model 1 = 0.02395521104335785\n",
            "Epoch 1 --> loss model 2 = 0.029603242874145508\n",
            "Epoch 1 --> loss model 3 = 0.01870221272110939\n",
            "Epoch 1 --> loss model 4 = 0.053473133593797684\n",
            "Epoch 1 --> loss model 5 = 0.09080306440591812\n",
            "Epoch 1 --> model 1 accuracy = 98.55769348144531\n",
            "Epoch 1 --> model 2 accuracy = 98.45753479003906\n",
            "Epoch 1 --> model 3 accuracy = 98.53765869140625\n",
            "Epoch 1 --> model 4 accuracy = 98.11698913574219\n",
            "Epoch 1 --> model 5 accuracy = 97.09535217285156\n",
            "Epoch 2 --> loss model 1 = 0.021150559186935425\n",
            "Epoch 2 --> loss model 2 = 0.02702346444129944\n",
            "Epoch 2 --> loss model 3 = 0.016133980825543404\n",
            "Epoch 2 --> loss model 4 = 0.04867418482899666\n",
            "Epoch 2 --> loss model 5 = 0.08316165953874588\n",
            "Epoch 2 --> model 1 accuracy = 98.64784240722656\n",
            "Epoch 2 --> model 2 accuracy = 98.56770324707031\n",
            "Epoch 2 --> model 3 accuracy = 98.70793151855469\n",
            "Epoch 2 --> model 4 accuracy = 98.10697174072266\n",
            "Epoch 2 --> model 5 accuracy = 97.20552825927734\n",
            "Epoch 3 --> loss model 1 = 0.020026959478855133\n",
            "Epoch 3 --> loss model 2 = 0.02535911649465561\n",
            "Epoch 3 --> loss model 3 = 0.015151706524193287\n",
            "Epoch 3 --> loss model 4 = 0.046722471714019775\n",
            "Epoch 3 --> loss model 5 = 0.08009926229715347\n",
            "Epoch 3 --> model 1 accuracy = 98.61778259277344\n",
            "Epoch 3 --> model 2 accuracy = 98.60777282714844\n",
            "Epoch 3 --> model 3 accuracy = 98.6278076171875\n",
            "Epoch 3 --> model 4 accuracy = 97.98677825927734\n",
            "Epoch 3 --> model 5 accuracy = 97.05528259277344\n",
            "Epoch 4 --> loss model 1 = 0.01819343864917755\n",
            "Epoch 4 --> loss model 2 = 0.023132042959332466\n",
            "Epoch 4 --> loss model 3 = 0.012878951616585255\n",
            "Epoch 4 --> loss model 4 = 0.04336756095290184\n",
            "Epoch 4 --> loss model 5 = 0.07464142143726349\n",
            "Epoch 4 --> model 1 accuracy = 98.5977554321289\n",
            "Epoch 4 --> model 2 accuracy = 98.41746520996094\n",
            "Epoch 4 --> model 3 accuracy = 98.56770324707031\n",
            "Epoch 4 --> model 4 accuracy = 98.07691955566406\n",
            "Epoch 4 --> model 5 accuracy = 97.59615325927734\n",
            "Epoch 5 --> loss model 1 = 0.01657753624022007\n",
            "Epoch 5 --> loss model 2 = 0.02053072117269039\n",
            "Epoch 5 --> loss model 3 = 0.011571056209504604\n",
            "Epoch 5 --> loss model 4 = 0.04026637226343155\n",
            "Epoch 5 --> loss model 5 = 0.06776461005210876\n",
            "Epoch 5 --> model 1 accuracy = 98.68789672851562\n",
            "Epoch 5 --> model 2 accuracy = 98.5977554321289\n",
            "Epoch 5 --> model 3 accuracy = 98.57772827148438\n",
            "Epoch 5 --> model 4 accuracy = 98.06690979003906\n",
            "Epoch 5 --> model 5 accuracy = 97.12539672851562\n"
          ]
        }
      ],
      "source": [
        "# Train and compare the 5 different architectures\n",
        "import numpy as np\n",
        "\n",
        "# Run with 2 different learning rate, momentum combinations\n",
        "hyperparameters = [[0.001, 0.01], [0.9, 0.9]]\n",
        "for hp in range(2):\n",
        "    # Run all the 5 models and evaluate them\n",
        "    lr = hyperparameters[0][hp]\n",
        "    momentum = hyperparameters[1][hp]\n",
        "    print(f\"Run with learning rate {lr} and momentum {momentum}\")\n",
        "    for epoch in range(n_epochs):\n",
        "        losses1 = []\n",
        "        losses2 = []\n",
        "        losses3 = []\n",
        "        losses4 = []\n",
        "        losses5 = []\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            y_pred1 = model1(inputs)\n",
        "            y_pred2 = model2(inputs)\n",
        "            y_pred3 = model3(inputs)\n",
        "            y_pred4 = model4(inputs)\n",
        "            y_pred5 = model5(inputs)\n",
        "            loss1 = loss_fn(y_pred1, labels)\n",
        "            loss2 = loss_fn(y_pred2, labels)\n",
        "            loss3 = loss_fn(y_pred3, labels)\n",
        "            loss4 = loss_fn(y_pred4, labels)\n",
        "            loss5 = loss_fn(y_pred5, labels)\n",
        "            losses1.append(loss1)\n",
        "            losses2.append(loss2)\n",
        "            losses3.append(loss3)\n",
        "            losses4.append(loss4)\n",
        "            losses5.append(loss5)\n",
        "            optimizer1.zero_grad()\n",
        "            optimizer2.zero_grad()\n",
        "            optimizer3.zero_grad()\n",
        "            optimizer4.zero_grad()\n",
        "            optimizer5.zero_grad()\n",
        "            loss1.backward()\n",
        "            loss2.backward()\n",
        "            loss3.backward()\n",
        "            loss4.backward()\n",
        "            loss5.backward()\n",
        "            optimizer1.step()\n",
        "            optimizer2.step()\n",
        "            optimizer3.step()\n",
        "            optimizer4.step()\n",
        "            optimizer5.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1} --> loss model 1 = {sum(losses1)/len(losses1)}')\n",
        "        print(f'Epoch {epoch + 1} --> loss model 2 = {sum(losses2)/len(losses2)}')\n",
        "        print(f'Epoch {epoch + 1} --> loss model 3 = {sum(losses3)/len(losses3)}')\n",
        "        print(f'Epoch {epoch + 1} --> loss model 4 = {sum(losses4)/len(losses4)}')\n",
        "        print(f'Epoch {epoch + 1} --> loss model 5 = {sum(losses5)/len(losses5)}')\n",
        "\n",
        "        acc = [0, 0, 0, 0, 0]\n",
        "        count = [0, 0, 0, 0, 0]\n",
        "        for inputs, labels in testloader:\n",
        "\n",
        "            y_pred1 = model1(inputs)\n",
        "            y_pred2 = model2(inputs)\n",
        "            y_pred3 = model3(inputs)\n",
        "            y_pred4 = model4(inputs)\n",
        "            y_pred5 = model5(inputs)\n",
        "\n",
        "            acc[0] += (torch.argmax(y_pred1, 1) == labels).float().sum()\n",
        "            acc[1] += (torch.argmax(y_pred2, 1) == labels).float().sum()\n",
        "            acc[2] += (torch.argmax(y_pred3, 1) == labels).float().sum()\n",
        "            acc[3] += (torch.argmax(y_pred4, 1) == labels).float().sum()\n",
        "            acc[4] += (torch.argmax(y_pred5, 1) == labels).float().sum()\n",
        "            for i in range(len(count)):\n",
        "                count[i] += len(labels)\n",
        "\n",
        "        for i in range(len(acc)):\n",
        "            acc[i] /= count[i]\n",
        "\n",
        "        for i in range(len(acc)):\n",
        "            print(f'Epoch {epoch + 1} --> model {i+1} accuracy = {acc[i]*100}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately because of the long computing time only 5 epochs and very limited learning rate / momentum combinations were possible. In general the less layers the higher the initial accuracy was. The size did not really make a big difference. For more than two layers the learning rate of 0.001 was to small and a learning rate of 0.01 lead to faster convergence. However the models performed all very well and the differences were only marginal."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
